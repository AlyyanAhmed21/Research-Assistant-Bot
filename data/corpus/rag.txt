Retrieval-Augmented Generation (RAG) is an architecture that enhances the accuracy and reliability of large language models (LLMs). It works by first retrieving relevant documents or passages from an external knowledge base, such as a vector database. Then, it provides these retrieved documents as context to the LLM along with the user's original query. This helps the model generate answers that are grounded in specific, verifiable information, reducing the chances of hallucination and allowing the model to answer questions about topics not included in its original training data.